{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Rubric\n",
    "\n",
    "Instructions: DELETE this cell before you submit via a `git push` to your repo before deadline. This cell is for your reference only and is not needed in your report. \n",
    "\n",
    " Scoring: Out of 10 points\n",
    "\n",
    "- Each Developing  => -2 pts\n",
    "- Each Unsatisfactory/Missing => -4 pts\n",
    "  - until the score is 0\n",
    "\n",
    "If students address the detailed feedback in a future checkpoint they will earn these points back\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "|                                  | **Unsatisfactory**                                                                                                                                                                                                                                                                                                                        | **Developing**                                                                                                                                                                                                       | **Proficient**                                                                                                                                                                                            | **Excellent**                                                                                                                                                                            |\n",
    "|----------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| **EDA relevance**                | EDA is mostly neither relevant to the question nor helpful in figuring out how to address the question. Or the EDA does address the question, but many obviously relevant variables / analyses / figures were not included. | EDA is partly irrelevant/unhelpful. EDA missed one or two obvioulsy relevant analysis (distributions of single variables or relationships between variables) | EDA includes the obviously relevant / helpful variables in addressing the question.                                                              | Thorough EDA fully explored the dataset                                                                                                                 |\n",
    "| **EDA analysis and description** | Many of the analyses are poor choices (e.g., using means instead of medians for obviously skewed data), or are poorly described in the text, or do not aid understanding the data                                                                                                                                                     | Some of the analyses are poor choices, or are poorly described in the text, or do not aid understanding the data                                                                                                 | All analyses are correct choices. Only one or two have minor issues in the text descriptions supporting them. Mostly they fit well with other elements of the EDA and support understanding the data  | All analyses are correct choices with clear text descriptions supporting them. The figures fit well with the other elements of the EDA, producing a clear understanding of the data. |\n",
    "| **EDA figures**                  | Many of the figures are poor plot choices (e.g., using a bar plot to represent a time series where it would be better to use a line plot) or have poor aesthetics (including colormap, data point shape/color, axis labels, titles, annotations, text legibility) or do not aid understanding the data                                | Some of the figures are poor plot choices or have poor aesthetics. Some figures do not aid understanding the data                                                                                                | All figures are correct plot choices. Only one or two have minor questionable aesthetic choices. The figures mostly fit well with the other elements of the EDA and support understanding the data    | All figures are correct plot choices with beautiful aesthetics. The figures fit well with the other elements of the EDA, producing a clear understanding of the data.                |\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 108 - EDA Checkpoint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n",
    "\n",
    "Instructions: REPLACE the contents of this cell with your team list and their contributions. Note that this will change over the course of the checkpoints\n",
    "\n",
    "This is a modified [CRediT taxonomy of contributions](https://credit.niso.org). For each group member please list how they contributed to this project using these terms:\n",
    "> Analysis, Background research, Conceptualization, Data curation, Experimental investigation, Methodology, Project administration, Software, Visualization, Writing – original draft, Writing – review & editing\n",
    "\n",
    "Example team list and credits:\n",
    "- Alice Anderson: Conceptualization, Data curation, Methodology, Writing - original draft\n",
    "- Bob Barker:  Analysis, Software, Visualization\n",
    "- Charlie Chang: Project administration, Software, Writing - review & editing\n",
    "- Dani Delgado: Analysis, Background research, Visualization, Writing - original draft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Question"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions: REPLACE the contents of this cell with your work, including any updates to recover points lost in your proposal feedback\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background and Prior Work"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions: REPLACE the contents of this cell with your work, including any updates to recover points lost in your proposal feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions: REPLACE the contents of this cell with your work, including any updates to recover points lost in your proposal feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data overview\n",
    "\n",
    "Instructions: REPLACE the contents of this cell with your work, including any updates to recover points lost in your data checkpoint feedback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code every time when you're actively developing modules in .py files.  It's not needed if you aren't making modules\n",
    "#\n",
    "## this code is necessary for making sure that any modules we load are updated here \n",
    "## when their source code .py files are modified\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Download Progress:   0%|                          | 0/3 [00:00<?, ?it/s]\n",
      "Downloading dataset1.csv:   0%|                     | 0.00/2.20k [00:00<?, ?B/s]\u001b[A\n",
      "Overall Download Progress:  33%|██████            | 1/3 [00:01<00:03,  1.63s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded: dataset1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading dataset2.csv:   0%|                     | 0.00/2.75M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading dataset2.csv:  21%|██▊          | 585k/2.75M [00:00<00:00, 5.83MB/s]\u001b[A\n",
      "Downloading dataset2.csv:  77%|█████████▏  | 2.11M/2.75M [00:00<00:00, 10.9MB/s]\u001b[A\n",
      "Overall Download Progress:  67%|████████████      | 2/3 [00:03<00:01,  1.66s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded: dataset2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading dataset3.csv:   0%|                     | 0.00/9.77M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading dataset3.csv:  11%|█▎          | 1.09M/9.77M [00:00<00:00, 10.1MB/s]\u001b[A\n",
      "Downloading dataset3.csv:  22%|██▌         | 2.10M/9.77M [00:00<00:00, 9.95MB/s]\u001b[A\n",
      "Downloading dataset3.csv:  37%|████▍       | 3.57M/9.77M [00:00<00:00, 12.1MB/s]\u001b[A\n",
      "Downloading dataset3.csv:  49%|█████▉      | 4.79M/9.77M [00:00<00:00, 12.1MB/s]\u001b[A\n",
      "Downloading dataset3.csv:  64%|███████▋    | 6.26M/9.77M [00:00<00:00, 12.9MB/s]\u001b[A\n",
      "Downloading dataset3.csv:  77%|█████████▎  | 7.56M/9.77M [00:00<00:00, 12.6MB/s]\u001b[A\n",
      "Downloading dataset3.csv:  94%|███████████▏| 9.14M/9.77M [00:00<00:00, 13.7MB/s]\u001b[A\n",
      "Overall Download Progress: 100%|██████████████████| 3/3 [00:08<00:00,  2.68s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded: dataset3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Setup code -- this only needs to be run once after cloning the repo!\n",
    "# this code downloads the data from its source to the `data/00-raw/` directory\n",
    "# if the data hasn't updated you don't need to do this again!\n",
    "\n",
    "# if you don't already have these packages (you should!) uncomment this line\n",
    "# %pip install requests tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append('./modules') # this tells python where to look for modules to import\n",
    "\n",
    "import get_data # this is where we get the function we need to download data\n",
    "\n",
    "# replace the urls and filenames in this list with your actual datafiles\n",
    "# yes you can use Google drive share links or whatever\n",
    "# format is a list of dictionaries; \n",
    "# each dict has keys of \n",
    "#   'url' where the resource is located\n",
    "#   'filename' for the local filename where it will be stored \n",
    "datafiles = [\n",
    "    { 'url': 'https://drive.google.com/uc?export=download&id=1Sp74YgcCWMykUZ5ZGgllfE-hxb6VzPRJ', 'filename':'dataset1.csv'},\n",
    "    { 'url': 'https://drive.google.com/uc?export=download&id=1UkbiqUGRhyRhUPCMYCTkAhGzPF2QOdYM', 'filename':'dataset2.csv'},\n",
    "    { 'url': 'https://drive.google.com/uc?export=download&id=1cFazIGXBgy7HeQLAC3ClZUK4dRVbxOZJ', 'filename':'dataset3.csv'}\n",
    "]\n",
    "\n",
    "get_data.get_raw(datafiles,destination_directory='data/00-raw/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset #1 \n",
    "\n",
    "Instructions: REPLACE the contents of this cell and the one below with your work, including any updates to recover points lost in your data checkpoint feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Included in at least 1 state law definition of \"Tobacco Product\"</th>\n",
       "      <th>Excise or Special Tax (non-sales tax)</th>\n",
       "      <th>Product Packaging</th>\n",
       "      <th>Youth Access / Other Retail Restrictions</th>\n",
       "      <th>Retail License / Permit Required</th>\n",
       "      <th>Smoke-Free Restrictions</th>\n",
       "      <th>Age</th>\n",
       "      <th>Military Exception Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arizona</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>18</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arkansas</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>21</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>California</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>21</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  \\\n",
       "0     Alabama   \n",
       "1      Alaska   \n",
       "2     Arizona   \n",
       "3    Arkansas   \n",
       "4  California   \n",
       "\n",
       "  Included in at least 1 state law definition of \"Tobacco Product\"  \\\n",
       "0                                                 No                 \n",
       "1                                                 No                 \n",
       "2                                                 No                 \n",
       "3                                                 No                 \n",
       "4                                                Yes                 \n",
       "\n",
       "  Excise or Special Tax (non-sales tax) Product Packaging  \\\n",
       "0                                   NaN               Yes   \n",
       "1                                   NaN               NaN   \n",
       "2                                   NaN               NaN   \n",
       "3                                   NaN               Yes   \n",
       "4                                   Yes               Yes   \n",
       "\n",
       "  Youth Access / Other Retail Restrictions Retail License / Permit Required  \\\n",
       "0                                      Yes                              Yes   \n",
       "1                                      Yes                              Yes   \n",
       "2                                      Yes                               No   \n",
       "3                                      Yes                              Yes   \n",
       "4                                      Yes                              Yes   \n",
       "\n",
       "  Smoke-Free Restrictions  Age  Military Exception Age  \n",
       "0                     Yes   21                     NaN  \n",
       "1                     Yes   19                     NaN  \n",
       "2                     Yes   18                     NaN  \n",
       "3                     Yes   21                    19.0  \n",
       "4                     Yes   21                    18.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## YOUR CODE TO LOAD/CLEAN/TIDY/WRANGLE THE DATA GOES HERE\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset from data/00-raw/\n",
    "policy_data = pd.read_csv(\"data/00-raw/dataset1.csv\")\n",
    "\n",
    "# View the dataset\n",
    "policy_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0',\n",
      "       'Included in at least 1 state law definition of \"Tobacco Product\"',\n",
      "       'Excise or Special Tax (non-sales tax)', 'Product Packaging',\n",
      "       'Youth Access / Other Retail Restrictions',\n",
      "       'Retail License / Permit Required', 'Smoke-Free Restrictions', 'Age',\n",
      "       'Military Exception Age'],\n",
      "      dtype='object')\n",
      "\n",
      "All columns passed: no nested data found. Dataset appears tidy.\n"
     ]
    }
   ],
   "source": [
    "# Check the columns of the dataset\n",
    "print(policy_data.columns)\n",
    "\n",
    "# Check for multiple values in a single cell (nested data)\n",
    "nested_found = False\n",
    "for col in policy_data.columns:\n",
    "    sample = policy_data[col].iloc[0]\n",
    "    if isinstance(sample, (list, dict)):\n",
    "        print(f'Column {col} may not be tidy; contains nested data.')\n",
    "        nested_found = True\n",
    "\n",
    "if not nested_found:\n",
    "    print(\"\\nAll columns passed: no nested data found. Dataset appears tidy.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51, 9)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Demonstrate the size of the dataset\n",
    "policy_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns to be concise\n",
    "policy_data.rename(columns={\"Unnamed: 0\": \"State\", 'Included in at least 1 state law definition of \"Tobacco Product\"': \"Included in State Definition\",\n",
    "                           \"Excise or Special Tax (non-sales tax)\": \"Tax\", \"Youth Access / Other Retail Restrictions\": \"Youth Restrictions\",\n",
    "                           \"Retail License / Permit Required\": \"Permit Required\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with missing data: 47\n",
      "\n",
      "Missing data per column:\n",
      "State                            0\n",
      "Included in State Definition     0\n",
      "Tax                             16\n",
      "Product Packaging               18\n",
      "Youth Restrictions               0\n",
      "Permit Required                  0\n",
      "Smoke-Free Restrictions          2\n",
      "Age                              0\n",
      "Military Exception Age          45\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check how much data is missing\n",
    "print(f\"Number of rows with missing data: {policy_data.isna().any(axis=1).sum()}\\n\")\n",
    "\n",
    "# Check where the data is missing\n",
    "print(f\"Missing data per column:\\n{policy_data.isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in Tax column: [nan 'Yes' 'No']\n",
      "Unique values in Product Packaging column: ['Yes' nan]\n",
      "Unique values in Smoke-Free Restrictions column: ['Yes' 'No' nan 'Yes ']\n",
      "Unique values in Military Exception Age column: [nan 19. 18.]\n"
     ]
    }
   ],
   "source": [
    "# Examine columns with missing values\n",
    "print(f\"Unique values in Tax column: {policy_data['Tax'].unique()}\")\n",
    "print(f\"Unique values in Product Packaging column: {policy_data['Product Packaging'].unique()}\")\n",
    "print(f\"Unique values in Smoke-Free Restrictions column: {policy_data['Smoke-Free Restrictions'].unique()}\")\n",
    "print(f\"Unique values in Military Exception Age column: {policy_data['Military Exception Age'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing data per column:\n",
      "State                           0\n",
      "Included in State Definition    0\n",
      "Tax                             0\n",
      "Product Packaging               0\n",
      "Youth Restrictions              0\n",
      "Permit Required                 0\n",
      "Smoke-Free Restrictions         0\n",
      "Age                             0\n",
      "Military Exception Age          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# In the binary Yes/No columns, fill missing data with unknown. In numerical columns, fill missing data with -1.\n",
    "policy_data = policy_data.fillna({\n",
    "    'Tax': 'Unk',\n",
    "    'Product Packaging': 'Unk',\n",
    "    'Smoke-Free Restrictions': 'Unk',\n",
    "    'Military Exception Age': -1})\n",
    "\n",
    "# Check missingness\n",
    "print(f\"Missing data per column:\\n{policy_data.isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in Smoke-Free Restrictions column: ['Yes' 'No' 'Unk' 'Yes ']\n"
     ]
    }
   ],
   "source": [
    "# When looking at unique values in each column, we noticed that Smoke-Free Restrictions looked interesting\n",
    "print(f\"Unique values in Smoke-Free Restrictions column: {policy_data['Smoke-Free Restrictions'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in Smoke-Free Restrictions column: ['Yes' 'No' 'Unk']\n"
     ]
    }
   ],
   "source": [
    "# Change Smoke-Free Restrictions column to remove whitespace\n",
    "policy_data['Smoke-Free Restrictions'] = policy_data['Smoke-Free Restrictions'].str.strip()\n",
    "\n",
    "# Verify that we now only have Yes, No, and Unk values\n",
    "print(f\"Unique values in Smoke-Free Restrictions column: {policy_data['Smoke-Free Restrictions'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                  Alabama\n",
       "1                   Alaska\n",
       "2                  Arizona\n",
       "3                 Arkansas\n",
       "4               California\n",
       "5                 Colorado\n",
       "6              Connecticut\n",
       "7                 Delaware\n",
       "8     District of Columbia\n",
       "9                  Florida\n",
       "10                 Georgia\n",
       "11                  Hawaii\n",
       "12                   Idaho\n",
       "13                Illinois\n",
       "14                 Indiana\n",
       "15                    Iowa\n",
       "16                  Kansas\n",
       "17                Kentucky\n",
       "18               Louisiana\n",
       "19                   Maine\n",
       "20                Maryland\n",
       "21           Massachusetts\n",
       "22                Michigan\n",
       "23               Minnesota\n",
       "24             Mississippi\n",
       "25                Missouri\n",
       "26                 Montana\n",
       "27                Nebraska\n",
       "28                  Nevada\n",
       "29           New Hampshire\n",
       "30              New Jersey\n",
       "31              New Mexico\n",
       "32                New York\n",
       "33          North Carolina\n",
       "34            North Dakota\n",
       "35                    Ohio\n",
       "36                Oklahoma\n",
       "37                  Oregon\n",
       "38            Pennsylvania\n",
       "39            Rhode Island\n",
       "40          South Carolina\n",
       "41            South Dakota\n",
       "42               Tennessee\n",
       "43                   Texas\n",
       "44                    Utah\n",
       "45                 Vermont\n",
       "46                Virginia\n",
       "47              Washington\n",
       "48           West Virginia\n",
       "49               Wisconsin\n",
       "50                 Wyoming\n",
       "Name: State, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This dataset should contain one row per US state. Why are there 51 rows?\n",
    "policy_data['State']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 9)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The dataset contains all 50 states as well as the District of Columbia. Remove that row.\n",
    "policy_data = policy_data.drop(index=8)\n",
    "\n",
    "# Reset index\n",
    "policy_data.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Look at shape of data\n",
    "policy_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make it easy to combine this dataset with our other datasets, we need a column with the abbreviation of each state\n",
    "# Create a dictionary to map state names to abbreviations\n",
    "state_dict = {\"Alabama\": \"AL\", \"Alaska\": \"AK\", \"Arizona\": \"AZ\", \"Arkansas\": \"AR\", \"California\": \"CA\", \"Colorado\": \"CO\", \"Connecticut\": \"CT\",\n",
    "    \"Delaware\": \"DE\", \"Florida\": \"FL\", \"Georgia\": \"GA\", \"Hawaii\": \"HI\", \"Idaho\": \"ID\", \"Illinois\": \"IL\", \"Indiana\": \"IN\", \"Iowa\": \"IA\",\n",
    "    \"Kansas\": \"KS\", \"Kentucky\": \"KY\", \"Louisiana\": \"LA\", \"Maine\": \"ME\", \"Maryland\": \"MD\", \"Massachusetts\": \"MA\", \"Michigan\": \"MI\",\n",
    "    \"Minnesota\": \"MN\", \"Mississippi\": \"MS\", \"Missouri\": \"MO\", \"Montana\": \"MT\", \"Nebraska\": \"NE\", \"Nevada\": \"NV\", \"New Hampshire\": \"NH\",\n",
    "    \"New Jersey\": \"NJ\", \"New Mexico\": \"NM\", \"New York\": \"NY\", \"North Carolina\": \"NC\", \"North Dakota\": \"ND\", \"Ohio\": \"OH\", \"Oklahoma\": \"OK\",\n",
    "    \"Oregon\": \"OR\", \"Pennsylvania\": \"PA\", \"Rhode Island\": \"RI\", \"South Carolina\": \"SC\", \"South Dakota\": \"SD\", \"Tennessee\": \"TN\", \"Texas\": \"TX\",\n",
    "    \"Utah\": \"UT\", \"Vermont\": \"VT\", \"Virginia\": \"VA\", \"Washington\": \"WA\", \"West Virginia\": \"WV\", \"Wisconsin\": \"WI\", \"Wyoming\": \"WY\"}\n",
    "\n",
    "# Create a new column in the DataFrame with state abbreviations\n",
    "policy_data['StateAbbr'] = policy_data['State'].map(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows: 0\n",
      "Dataset shape: (50, 10)\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicate rows\n",
    "print(f\"Number of duplicate rows: {policy_data.duplicated().sum()}\")\n",
    "\n",
    "# Check the final shape of the dataset\n",
    "print(f\"Dataset shape: {policy_data.shape}\")\n",
    "\n",
    "# Save final wrangled data\n",
    "policy_data.to_csv(\"data/02-processed/policy_data_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset #2\n",
    " as above, add any more copies of this that you need to given how many datasets you have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE TO LOAD/CLEAN/TIDY/WRANGLE THE DATA GOES HERE\n",
    "# Load the dataset from data/00-raw/\n",
    "yts_data = pd.read_csv(\"data/00-raw/dataset2.csv\")\n",
    "\n",
    "# View the dataset\n",
    "yts_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the columns of the dataset\n",
    "print(yts_data.columns)\n",
    "\n",
    "# Check for multiple values in a single cell (nested data)\n",
    "nested_found = False\n",
    "for col in yts_data.columns:\n",
    "    sample = yts_data[col].iloc[0]\n",
    "    if isinstance(sample, (list, dict)):\n",
    "        print(f'Column {col} may not be tidy; contains nested data.')\n",
    "        nested_found = True\n",
    "\n",
    "if not nested_found:\n",
    "    print(\"\\nAll columns passed: no nested data found. Dataset appears tidy.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the size of the dataset\n",
    "yts_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how much data is missing\n",
    "print(f\"Number of rows with missing data: {yts_data.isna().any(axis=1).sum()}\\n\")\n",
    "\n",
    "# Check where the data is missing\n",
    "print(f\"Missing data per column:\\n{yts_data.isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine columns with lots of missing values\n",
    "print(f\"Unique values in Data_Value_Footnote column: {yts_data['Data_Value_Footnote'].unique()}\\n\")\n",
    "print(f\"Unique values in Data_Value_Footnote_Symbol column: {yts_data['Data_Value_Footnote_Symbol'].unique()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looks like in this case the presence of a footnote and footnote symbol indicates suppressed data\n",
    "# To verify this claim, create a DataFrame of only rows that have a footnote\n",
    "footnote_rows = yts_data[yts_data['Data_Value_Footnote'] == 'Data in these cells have been suppressed because of a small sample size']\n",
    "\n",
    "# Check the shape of the new DataFrame\n",
    "print(f\"Shape of footnote_rows DataFrame: {footnote_rows.shape}\\n\")\n",
    "\n",
    "# Check for missingness in the new DataFrame\n",
    "# This shows that every row that contains a footnote has missing values in 6 other columns\n",
    "print(f\"Missing data per column in footnote_rows DataFrame:\\n{footnote_rows.isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, in our original DataFrame, we know that we can safely drop rows that have a footnote, since those rows contain missing data\n",
    "yts_data = yts_data[yts_data['Data_Value_Footnote'] != 'Data in these cells have been suppressed because of a small sample size']\n",
    "\n",
    "# We can also drop the Data_Value_Footnote and Data_Value_Footnote_Symbol columns because they no longer contain meaningful information\n",
    "yts_data.drop(columns=['Data_Value_Footnote', 'Data_Value_Footnote_Symbol'], inplace=True)\n",
    "\n",
    "# Save interim progress\n",
    "yts_data.to_csv(\"data/01-interim/yts_data_removed_footnotes.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missingness again\n",
    "yts_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate the rows that have missing values in the Response column\n",
    "missing_response = yts_data[yts_data['Response'].isna()]\n",
    "\n",
    "# Examine TopicDesc column\n",
    "print(f\"Unique values in TopicDesc column for all data: {yts_data['TopicDesc'].unique()}\\n\")\n",
    "print(f\"Unique values in TopicDesc column among rows with missing data in Response: {missing_response['TopicDesc'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All of the rows with missing data in the Response column correspond with the string 'Cessation (Youth)' in the TopicDesc column\n",
    "# Fill missing values in the Response column with a meaningful placeholder\n",
    "yts_data['Response'] = yts_data['Response'].fillna('Quit')\n",
    "\n",
    "# Drop all other rows with missing values\n",
    "yts_data = yts_data.dropna()\n",
    "\n",
    "# Examine missingness\n",
    "print(f\"{yts_data.isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows\n",
    "print(f\"Number of duplicate rows: {yts_data.duplicated().sum()}\")\n",
    "\n",
    "# Drop ID columns that are not relevant to our analysis\n",
    "yts_data = yts_data.drop(['TopicTypeId', 'TopicId', 'MeasureId', 'StratificationID1', 'StratificationID2', 'StratificationID3',\n",
    "                         'StratificationID4', 'SubMeasureID', 'DisplayOrder'], axis=1)\n",
    "\n",
    "# Rename LocationAbbr column to make it easier to combine with other datasets\n",
    "yts_data.rename(columns={'LocationAbbr': 'StateAbbr'}, inplace=True)\n",
    "\n",
    "# Check the final shape of the dataset\n",
    "print(f\"Dataset shape: {yts_data.shape}\")\n",
    "\n",
    "# Save final wrangled data\n",
    "yts_data.to_csv(\"data/02-processed/yts_data_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for numerical columns\n",
    "numeric_cols = ['Data_Value', 'Data_Value_Std_Err', 'Low_Confidence_Limit', 'High_Confidence_Limit', 'Sample_Size']\n",
    "yts_data[numeric_cols].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset #3\n",
    " as above, add any more copies of this that you need to given how many datasets you have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE TO LOAD/CLEAN/TIDY/WRANGLE THE DATA GOES HERE\n",
    "\n",
    "# Load the dataset from data/00-raw/\n",
    "tweet_data = pd.read_csv(\"data/00-raw/dataset3.csv\", index_col=0)\n",
    "\n",
    "# View the dataset\n",
    "tweet_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the columns of the dataset\n",
    "print(tweet_data.columns)\n",
    "\n",
    "# Check for multiple values in a single cell (nested data)\n",
    "nested_found = False\n",
    "for col in tweet_data.columns:\n",
    "    sample = tweet_data[col].iloc[0]\n",
    "    if isinstance(sample, (list, dict)):\n",
    "        print(f'Column {col} may not be tidy; contains nested data.')\n",
    "        nested_found = True\n",
    "\n",
    "if not nested_found:\n",
    "    print(\"\\nAll columns passed: no nested data found. Dataset appears tidy.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the size of the dataset\n",
    "tweet_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how much data is missing\n",
    "print(f\"Number of rows with missing data: {tweet_data.isna().any(axis=1).sum()}\\n\")\n",
    "\n",
    "# Check where the data is missing\n",
    "print(f\"Missing data per column:\\n{tweet_data.isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looks like every single row has missing data in the TweetCoordinate column. Let's remove that column.\n",
    "tweet_data.drop('TweetCoordinate', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets investigate the data that has missing values in TweetState\n",
    "missing_state_data = tweet_data[tweet_data['TweetState'].isna()]\n",
    "missing_state_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There doesn't appear to be a pattern among the data that is missing TweetState, so we will just drop rows that have missing values\n",
    "tweet_data = tweet_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's look more closely at the TweetState column\n",
    "print(f\"Unique values in the TweetState column:\\n{tweet_data['TweetState'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For our analysis, we only want to include tweets that were posted in the US\n",
    "# Create a list of all US state abbreviations\n",
    "states_list = [\"AK\", \"AL\", \"AR\", \"AZ\", \"CA\", \"CO\", \"CT\", \"DE\", \"FL\", \"GA\", \"HI\", \"IA\",\n",
    "    \"ID\", \"IL\", \"IN\", \"KS\", \"KY\", \"LA\", \"MA\", \"MD\", \"ME\", \"MI\", \"MN\", \"MO\",\n",
    "    \"MS\", \"MT\", \"NC\", \"ND\", \"NE\", \"NH\", \"NJ\", \"NM\", \"NV\", \"NY\", \"OH\", \"OK\",\n",
    "    \"OR\", \"PA\", \"RI\", \"SC\", \"SD\", \"TN\", \"TX\", \"UT\", \"VA\", \"VT\", \"WA\", \"WI\",\n",
    "    \"WV\", \"WY\"]\n",
    "\n",
    "# Only keep rows where TweetState is a US state\n",
    "tweet_data = tweet_data[tweet_data['TweetState'].isin(states_list)]\n",
    "\n",
    "# Check unique values in TweetState column to verify that they are all US states\n",
    "print(f\"Unique values in the TweetState column:\\n{tweet_data['TweetState'].unique()}\\n\")\n",
    "\n",
    "# Check that there are 50 unique values in the TweetState column\n",
    "print(f\"Number of unique values in the TweetState column:\\n{tweet_data['TweetState'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows\n",
    "print(f\"Number of duplicate rows: {tweet_data.duplicated().sum()}\")\n",
    "\n",
    "# Check the final shape of the dataset\n",
    "print(f\"Dataset shape: {tweet_data.shape}\")\n",
    "\n",
    "# Rename TweetState column to StateAbbr to make it easier to combine with other datasets\n",
    "tweet_data.rename(columns={'TweetState': 'StateAbbr'}, inplace=True)\n",
    "\n",
    "# Save final wrangled data\n",
    "tweet_data.to_csv(\"data/02-processed/tweet_data_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distribution of sentiment scores\n",
    "# The plot shows that there are many tweets with either positive or negative sentiment, meaning that sentiment varies within the dataset\n",
    "sns.countplot(x='VADER_sentiment_class', data=tweet_data)\n",
    "plt.title(\"Distribution of Tweet Sentiment\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "### Exploratory Data Analysis\n",
    "\n",
    "Instructions: replace the words in this subsection with whatever words you need to setup and preview the EDA you're going to do.   \n",
    "\n",
    "Please explicitly load the fully wrangled data you will use from `data/02-processed`.  This is a good idea rather than forcing people to re-run the data getting / wrangling cells above.  Sometimes it takes a long time to get / wrangle data compared to reloading the fixed up dataset.\n",
    "\n",
    "Carry out whatever EDA you need to for your project in the code cells below.  Because every project will be different we can't really give you much of a template at this point. But please make sure you describe the what and why in text here as well as providing interpretation of results and context.\n",
    "\n",
    "Please note that you should consider the use of python modules in your work.  Any code which gets called repeatedly should be modularized. So if you run the same pre-processing, analysis or visualiazation on different subsets of the data, then you should turn that into a function or class.  Put that function or class in a .py file that lives in `modules/`.  Import the module you made and use it to get your work done.  For reference see `get_raw()` which is inside `modules/get_data.py`. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 1 of EDA - please give it a better title than this\n",
    "\n",
    "Some more words and stuff.  Remember notebooks work best if you interleave the code that generates a result with properly annotate figures and text that puts these results into context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "policy_eda_data = pd.read_csv(\"data/02-processed/policy_data_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 2 of EDA - please give it a better title than this\n",
    "\n",
    "Some more words and stuff.  Remember notebooks work best if you interleave the code that generates a result with properly annotate figures and text that puts these results into context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "yts_eda_data = pd.read_csv(\"data/02-processed/yts_data_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 3 of EDA - please give it a better title than this\n",
    "\n",
    "Some more words and stuff.  Remember notebooks work best if you interleave the code that generates a result with properly annotate figures and text that puts these results into context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "tweet_eda_data = pd.read_csv(\"data/02-processed/tweet_data_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 4 of EDA - please give it a better title than this\n",
    "\n",
    "Some more words and stuff.  Remember notebooks work best if you interleave the code that generates a result with properly annotate figures and text that puts these results into context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ethics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions: REPLACE the contents of this cell with your work, including any updates to recover points lost in your proposal feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Expectations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions: REPLACE the contents of this cell with your work, including any updates to recover points lost in your proposal feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Timeline Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions: Replace this with your timeline.  **PLEASE UPDATE your Timeline!** No battle plan survives contact with the enemy, so make sure we understand how your plans have changed.  Also if you have lost points on the previous checkpoint fix them"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "16b860a9f5fc21240e9d88c0ee13691518c3ce67be252e54a03b9b5b11bd3c7a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
